---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Channel-temporal attention for first-person video domain adaptation
subtitle: ''
summary: ''
authors:
- Xianyuan Liu
- Shuo Zhou
- Tao Lei
- Haiping Lu
tags: []
categories: []
date: '2021-08-01'
lastmod: 2021-12-29T13:50:34Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-12-29T13:50:33.087274Z'
publication_types:
- '3'
abstract: 'Unsupervised Domain Adaptation (UDA) can transfer knowledge from labeled source data to unlabeled target data of the same categories. However, UDA for first-person action recognition is an under-explored problem, with lack of datasets and limited consideration of first-person video characteristics. This paper focuses on addressing this problem. Firstly, we propose two small-scale first-person video domain adaptation datasets: ADLsmall and GTEA-KITCHEN. Secondly, we introduce channel-temporal attention blocks to capture the channel-wise and temporal-wise relationships and model their inter-dependencies important to first-person vision. Finally, we propose a Channel-Temporal Attention Network (CTAN) to integrate these blocks into existing architectures. CTAN outperforms baselines on the two proposed datasets and one existing dataset EPICcvpr20'
publication: '*arXiv preprint arXiv:2108.07846*'
links:
- name: Link
  url: https://arxiv.org/abs/2108.07846
---
